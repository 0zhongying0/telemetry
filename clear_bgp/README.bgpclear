### NOTE:  Leaf4 has a bad optic in port Hu0/0/0/16 and is in a shutdown state for this test
### so you will not see telemetry records for this port  in the data set.

The bgp clear test clears all BGP information to represent a BGP process crash and recover.  
This clears 
 1) neighbors; 
 2) all learned and programmed routes other than attached;
 3) all learned and programmed paths

The test scenario has ~1.3Tbps of application traffic traversing the fabric.  Traffic scenarios 
included sustained:
 1) TCP flows;
 2) G.711a IP voice call (64 kbps per call);
 3) RTP Blue Ray movie (45 Mbps per stream);
 4) Skype-1050P stream;
 5) AMR-WB VOIP (23.85 kbps per call);
 6) YouTube 4K video stream

The differing application flow/stream types give us the ability to see the various performance metric 
changes for the applications.  Items such as jitter, MOS for voice, drops for each of the various flows and protocols, 
etc.  

This dataset reflects the impacts of clearing all bgp information at various places throughput our CLOS fabric. 
The first four tests effect positions in the leaf spine leaf of the clos.  Each leaf having a different number
of downstream connections to edge routers and top of rack switches.   The spine tests will show the effect of traffic
redistribution over the remaining ECMP paths but will push the traffic rehashing back to the leafs.

Event 1 leaf1
Event 2 leaf6
Event 3 spine2  
Event 4 cascade bgp clear of leaf2 followed by leaf8 30 seconds later
Event 5 casecade bgp clear of spine1 followed by spine2 and then spine3 with ~45 second gap between each event.

The last two tests have multiple devices in the event.  The first has two leafs, one on either side of the spine which
will force traffic changes back to the ToR and edge routers as well as effect traffic at the spines.  The next test effects
three spines to simulate a bgp process crash and restart cascading across three spines. This last test forces all traffic
to a single spine, effecting rehashing on all leafs.


#### Metric Information ####

Please Note: 

Are sampled per interface
 leaf and spine have 32 network interfaces, 1 management interface and 1 Null0 interface
 dr have 72 network interfaces (36 per line card), 1 management interface and 1 Null0 interface

The topology is configured for a dual stack environment with both IPv6 and IPv4 address families
_________________________


Cisco-IOS-XR-fib-common-oper:fib-statistics/nodes/node/drops
Cisco-IOS-XR-nto-misc-oper:memory-summary/nodes/node/summary
Cisco-IOS-XR-wdsysmon-fd-oper:system-monitoring/cpu-utilization

Are sampled per CPU
 leaf and spine have one CPU (0/RP0/CPU0)
 dr have N CPUs where N includes the CPU per line card and per RP
  in this sample case there are 3 CPU (0/RP0/CPU0, 0/0/CPU/0, 0/1/CPU/0)


________________________

Cisco-IOS-XR-ip-rib-ipv4-oper:rib/vrfs/vrf/afs/af/safs/saf/ip-rib-route-table-names/ip-rib-route-table-name/protocol/bgp/as/information
Cisco-IOS-XR-ipv4-bgp-oper:bgp/instances/instance/instance-active/default-vrf/process-info
  
Are sampled per device
